# Pytorch Advanced

## 1. VGG

### 1.1 학습된 VGG 모델을 사용하는 방법
- VGG-16 모델은 2014년 ILSVRC에서 2위를 차지
- 구성이 간단하여 다양한 딥러닝 응용  기술의 기반 네트워크로 사용됨
- 층 수에 FC layer, Convolution layer는 포함되나 활성화함수, 풀링층, 드롭아웃 등은 포함되지 않음
- 파이토치와 필로는 이미지 구성 차원 순서가 다름
  - 파이토치: (채널, 높이, 너비)
  - 필로: (높이, 너비, 채널)
  - `img_transformed = img_transformed.numpy().transpose((1, 2, 0))`
- `__call__()`은 파이썬의 일반 메서드. 구체적 함수를 지정하지 않고 호출하면 실행되는 함수
- torch.detach(): 출력값을 네트워크에서 분리
- 파이토치 네트워크에 이미지를 입력할 때 데이터는 미니배치 형태가 되어야 함
  - `unsqueeze_(0)` 메소드로 입력 데이터에 미니배치 차원 추가

### 1.2 파이토치를 활용한 딥러닝 구현 흐름
- 파이토치 딥러닝 구현 흐름
   1. 전처리, 후처리, 네트워크 모델의 입출력 확인
   2. 데이터셋 작성
       - 입력 데이터와 라벨 등을 쌍으로 갖는 클래스
       - 전처리 클래스의 인스턴스를 할당하여, 파일을 읽을 때 자동으로 전처리 적용
   3. 데이터 로더 작성
       - 데이터를 어떻게 가져올지 설정하는 클래스
       - Dataset에서 미니배치를 쉽게 가져오도록 설정         
   4. 네트워크 모델 작성
   5. 순전파 정의
   6. 손실함수 정의
   7. 최적화 기법 설정
   8. 학습/검증 실시
   9. 테스트 데이터로 추론


### 1-3. 전이학습 구현
- 데이터셋에서 augmentation도 함
  - 에폭마다 다르게 적용
  - ex) resized crop, horizontal flip ...

- glob을 이용하여 하위 디렉토리 파일 경로를 불러올 수 있음

- Dataset 작성 시, 기존 Pytorch의 Dataset 상속
  - 이때 `__len__`과 `__getitem__` 메서드 구현해야 함

- 메모리가 부족한 경우 batch_size를 작게 수행해야 함
- DataLoader 사용 시, train 시에는 shuffle=True, validation 시에는 shuffle=False

- 전이학습으로 학습하고 변화시킬 파라미터를 설정해야 함
  - ex. `update_param_names = ["classifier.6.weight", "classifier.6.bias"]`
- 학습시킬 파라미터 외에는 경사를 계산하지 않고 변하지 않도록 설정 -> 반드시 이래야 할까?
  - `param.requires_grad = False`

- 학습하지 않을 시 검증 선능을 확인하기 위해 epoch=0의 훈련 생략
- 학습 시 손실은 미니배치 크기의 평균 값으로 나옴. 따라서 미니배치 크기를 곱해 미니배치의 총 손실을 구함
  - `epoch_loss += loss.item() * inputs.size(0)`


- AWS AMI에 딥러닝을 위한 'Deep Learning AMI'가 있음
  - 파이토치, 텐서플로, 케라스. CUDA 등 패키지가 아나콘다와 함께 이미 설치돼있음
- 주피터 노트북은 기본적으로 8888번 포트에서 시작됨
  - 로컬환경과 클라우드 환경이 겹치지 않게 하려면 포트를 다르게 설정하면 좋음(ex. 9999포트)


- 파인튜닝과 전이학습은 다른 것
  - 전이학습은 출력층만 재학습
  - 파인튜닝은 모든 층의 파라미터 재학습
    - 일반적으로 입력층에 가까운 부분의 파라미터는 학습률 작게
    - 출력층에 가까운 부분의 파라미터는 학습률 크게
    - 